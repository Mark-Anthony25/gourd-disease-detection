{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14855092,"sourceType":"datasetVersion","datasetId":9501962}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nimport os, shutil, random\nfrom pathlib import Path\n\nBASE = \"/kaggle/input/leaf-image-dataset-for-disease-detection-in-bitter/Leaf Image Dataset for Disease Detection in Bitter/Dataset/Dataset\"\nRAW = os.path.join(BASE, \"Raw Data\")\nAUG = os.path.join(BASE, \"Augmented Data\")\n\nOUT = \"/kaggle/working/working_dataset\"\n\nrandom.seed(42)\n\ndef copy_images(src_list, dst_dir):\n    os.makedirs(dst_dir, exist_ok=True)\n    for f in src_list:\n        shutil.copy(f, dst_dir)\n\nfor split in [\"train\",\"val\",\"test\"]:\n    shutil.rmtree(os.path.join(OUT,split), ignore_errors=True)\n\n# iterate crops\nfor crop in os.listdir(RAW):\n    crop_path = os.path.join(RAW, crop)\n\n    for disease in os.listdir(crop_path):\n        raw_class = os.path.join(crop_path, disease)\n\n        images = [os.path.join(raw_class,i) for i in os.listdir(raw_class)]\n\n        random.shuffle(images)\n\n        n = len(images)\n        train_raw = images[:int(n*0.7)]\n        val_raw   = images[int(n*0.7):int(n*0.85)]\n        test_raw  = images[int(n*0.85):]\n\n        # copy RAW splits\n        copy_images(train_raw, f\"{OUT}/train/{crop}/{disease}\")\n        copy_images(val_raw,   f\"{OUT}/val/{crop}/{disease}\")\n        copy_images(test_raw,  f\"{OUT}/test/{crop}/{disease}\")\n\n        # add AUGMENTED only to TRAIN\n        aug_class = os.path.join(AUG, crop, disease)\n        if os.path.exists(aug_class):\n            aug_imgs = [os.path.join(aug_class,i) for i in os.listdir(aug_class)]\n            copy_images(aug_imgs, f\"{OUT}/train/{crop}/{disease}\")\n\nprint(\"DONE SPLITTING\")\n\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-16T07:42:50.167068Z","iopub.execute_input":"2026-02-16T07:42:50.167345Z","iopub.status.idle":"2026-02-16T07:42:50.176687Z","shell.execute_reply.started":"2026-02-16T07:42:50.167310Z","shell.execute_reply":"2026-02-16T07:42:50.176016Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'\\nimport os, shutil, random\\nfrom pathlib import Path\\n\\nBASE = \"/kaggle/input/leaf-image-dataset-for-disease-detection-in-bitter/Leaf Image Dataset for Disease Detection in Bitter/Dataset/Dataset\"\\nRAW = os.path.join(BASE, \"Raw Data\")\\nAUG = os.path.join(BASE, \"Augmented Data\")\\n\\nOUT = \"/kaggle/working/working_dataset\"\\n\\nrandom.seed(42)\\n\\ndef copy_images(src_list, dst_dir):\\n    os.makedirs(dst_dir, exist_ok=True)\\n    for f in src_list:\\n        shutil.copy(f, dst_dir)\\n\\nfor split in [\"train\",\"val\",\"test\"]:\\n    shutil.rmtree(os.path.join(OUT,split), ignore_errors=True)\\n\\n# iterate crops\\nfor crop in os.listdir(RAW):\\n    crop_path = os.path.join(RAW, crop)\\n\\n    for disease in os.listdir(crop_path):\\n        raw_class = os.path.join(crop_path, disease)\\n\\n        images = [os.path.join(raw_class,i) for i in os.listdir(raw_class)]\\n\\n        random.shuffle(images)\\n\\n        n = len(images)\\n        train_raw = images[:int(n*0.7)]\\n        val_raw   = images[int(n*0.7):int(n*0.85)]\\n        test_raw  = images[int(n*0.85):]\\n\\n        # copy RAW splits\\n        copy_images(train_raw, f\"{OUT}/train/{crop}/{disease}\")\\n        copy_images(val_raw,   f\"{OUT}/val/{crop}/{disease}\")\\n        copy_images(test_raw,  f\"{OUT}/test/{crop}/{disease}\")\\n\\n        # add AUGMENTED only to TRAIN\\n        aug_class = os.path.join(AUG, crop, disease)\\n        if os.path.exists(aug_class):\\n            aug_imgs = [os.path.join(aug_class,i) for i in os.listdir(aug_class)]\\n            copy_images(aug_imgs, f\"{OUT}/train/{crop}/{disease}\")\\n\\nprint(\"DONE SPLITTING\")\\n'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import tensorflow as tf\n\nIMG = 224\nBATCH = 16\nAUTOTUNE = tf.data.AUTOTUNE\n\ndef load_dataset(path, shuffle=True):\n\n    ds = tf.keras.utils.image_dataset_from_directory(\n        path,\n        image_size=(IMG,IMG),\n        batch_size=BATCH,\n        shuffle=shuffle\n    )\n\n    class_names = ds.class_names\n    ds = ds.prefetch(AUTOTUNE)\n\n    return ds, class_names\n\n\ntrain_crop, CROP_NAMES = load_dataset(\"/kaggle/working/working_dataset/train\")\nval_crop, _ = load_dataset(\"/kaggle/working/working_dataset/val\")\ntest_crop, _ = load_dataset(\"/kaggle/working/working_dataset/test\", shuffle=False)\n\nprint(\"CROPS:\", CROP_NAMES)\nNUM_CROPS = len(CROP_NAMES)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T07:48:27.407644Z","iopub.execute_input":"2026-02-16T07:48:27.408281Z","iopub.status.idle":"2026-02-16T07:48:27.909689Z","shell.execute_reply.started":"2026-02-16T07:48:27.408251Z","shell.execute_reply":"2026-02-16T07:48:27.909140Z"}},"outputs":[{"name":"stdout","text":"Found 13839 files belonging to 4 classes.\nFound 684 files belonging to 4 classes.\nFound 690 files belonging to 4 classes.\nCROPS: ['Bitter gourd', 'Okra', 'Pumpkin', 'Ridge gourd']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\n\nBASE = \"/kaggle/working/working_dataset\"\n\ndef count_images(path):\n    total = 0\n    for root,_,files in os.walk(path):\n        total += len([f for f in files if f.lower().endswith(('.jpg','.png','.jpeg'))])\n    return total\n\nfor split in [\"train\",\"val\",\"test\"]:\n    print(f\"\\n{split.upper()}\")\n    for crop in os.listdir(os.path.join(BASE,split)):\n        for disease in os.listdir(os.path.join(BASE,split,crop)):\n            p = os.path.join(BASE,split,crop,disease)\n            print(crop,\"|\",disease,\"=\",len(os.listdir(p)))\n\nprint(\"\\nTOTAL COUNTS\")\nfor s in [\"train\",\"val\",\"test\"]:\n    print(s, count_images(os.path.join(BASE,s)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T07:48:31.432838Z","iopub.execute_input":"2026-02-16T07:48:31.433473Z","iopub.status.idle":"2026-02-16T07:48:31.462541Z","shell.execute_reply.started":"2026-02-16T07:48:31.433444Z","shell.execute_reply":"2026-02-16T07:48:31.461790Z"}},"outputs":[{"name":"stdout","text":"\nTRAIN\nRidge gourd | Downey Mildew = 383\nRidge gourd | Healthy = 355\nOkra | Cerospora Leaf Spot = 3078\nOkra | Healthy = 3089\nPumpkin | Downey Mildew = 2918\nPumpkin | Healthy = 3049\nBitter gourd | Downey Mildew = 330\nBitter gourd | Anthracnose = 287\nBitter gourd | Healthy = 350\n\nVAL\nRidge gourd | Downey Mildew = 82\nRidge gourd | Healthy = 76\nOkra | Cerospora Leaf Spot = 81\nOkra | Healthy = 81\nPumpkin | Downey Mildew = 77\nPumpkin | Healthy = 80\nBitter gourd | Downey Mildew = 71\nBitter gourd | Anthracnose = 61\nBitter gourd | Healthy = 75\n\nTEST\nRidge gourd | Downey Mildew = 83\nRidge gourd | Healthy = 77\nOkra | Cerospora Leaf Spot = 81\nOkra | Healthy = 82\nPumpkin | Downey Mildew = 77\nPumpkin | Healthy = 81\nBitter gourd | Downey Mildew = 71\nBitter gourd | Anthracnose = 62\nBitter gourd | Healthy = 76\n\nTOTAL COUNTS\ntrain 13839\nval 684\ntest 690\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip -q install keras-tuner\nimport keras_tuner as kt\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T07:49:24.095537Z","iopub.execute_input":"2026-02-16T07:49:24.095835Z","iopub.status.idle":"2026-02-16T07:49:28.376865Z","shell.execute_reply.started":"2026-02-16T07:49:24.095810Z","shell.execute_reply":"2026-02-16T07:49:28.376169Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def build_crop_model(hp):\n\n    base = tf.keras.applications.EfficientNetB0(\n        include_top=False,\n        weights=\"imagenet\",\n        input_shape=(IMG,IMG,3)\n    )\n\n    # Tune fine-tuning\n    base.trainable = hp.Boolean(\"fine_tune\")\n\n    x = layers.GlobalAveragePooling2D()(base.output)\n\n    # Tune classifier size\n    x = layers.Dense(\n        hp.Choice(\"dense_units\",[64,128,256]),\n        activation=\"relu\")(x)\n\n    x = layers.Dropout(\n        hp.Float(\"dropout\",0.2,0.6,step=0.1))(x)\n\n    out = layers.Dense(NUM_CROPS,activation=\"softmax\")(x)\n\n    model = keras.Model(base.input,out)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(\n            hp.Choice(\"lr\",[1e-3,3e-4,1e-4])),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T07:49:37.241724Z","iopub.execute_input":"2026-02-16T07:49:37.243150Z","iopub.status.idle":"2026-02-16T07:49:37.248999Z","shell.execute_reply.started":"2026-02-16T07:49:37.243103Z","shell.execute_reply":"2026-02-16T07:49:37.248241Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"tuner = kt.Hyperband(\n    build_crop_model,\n    objective=\"val_accuracy\",\n    max_epochs=8,\n    directory=\"automl_crop\",\n    project_name=\"crop_classifier\"\n)\n\ntuner.search(train_crop, validation_data=val_crop)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T07:49:55.374862Z","iopub.execute_input":"2026-02-16T07:49:55.375218Z","iopub.status.idle":"2026-02-16T08:07:15.783202Z","shell.execute_reply.started":"2026-02-16T07:49:55.375190Z","shell.execute_reply":"2026-02-16T08:07:15.782100Z"}},"outputs":[{"name":"stdout","text":"Trial 4 Complete [00h 04m 53s]\nval_accuracy: 1.0\n\nBest val_accuracy So Far: 1.0\nTotal elapsed time: 00h 16m 38s\n\nSearch: Running Trial #5\n\nValue             |Best Value So Far |Hyperparameter\nFalse             |True              |fine_tune\n128               |256               |dense_units\n0.3               |0.4               |dropout\n0.0001            |0.0001            |lr\n3                 |3                 |tuner/epochs\n0                 |0                 |tuner/initial_epoch\n1                 |1                 |tuner/bracket\n0                 |0                 |tuner/round\n\nEpoch 1/3\n\u001b[1m843/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8731 - loss: 0.3645","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_317/3930123373.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_crop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_crop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         if self.oracle.get_trial(trial.trial_id).metrics.exists(\n\u001b[1;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_tuner/src/tuners/hyperband.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tuner/epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"initial_epoch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tuner/initial_epoch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_hypermodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mobj_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# Save the build config for model loading later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_tuner/src/engine/hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m   function = trace_function(\n\u001b[0m\u001b[1;32m    133\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    228\u001b[0m   )\n\u001b[1;32m    229\u001b[0m   lookup_func_type, lookup_func_context = (\n\u001b[0;32m--> 230\u001b[0;31m       function_type_utils.make_canonicalized_monomorphic_type(\n\u001b[0m\u001b[1;32m    231\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m           \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py\u001b[0m in \u001b[0;36mmake_canonicalized_monomorphic_type\u001b[0;34m(args, kwargs, capture_types, polymorphic_type)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m   function_type, type_context = (\n\u001b[0;32m--> 375\u001b[0;31m       function_type_lib.canonicalize_to_monomorphic(\n\u001b[0m\u001b[1;32m    376\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolymorphic_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36mcanonicalize_to_monomorphic\u001b[0;34m(args, kwargs, default_values, capture_types, polymorphic_type)\u001b[0m\n\u001b[1;32m    554\u001b[0m   \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m   \u001b[0mtype_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInternalTracingContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m   has_var_positional = any(p.kind is Parameter.VAR_POSITIONAL\n\u001b[0m\u001b[1;32m    557\u001b[0m                            for p in polymorphic_type.parameters.values())\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    554\u001b[0m   \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m   \u001b[0mtype_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInternalTracingContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m   has_var_positional = any(p.kind is Parameter.VAR_POSITIONAL\n\u001b[0m\u001b[1;32m    557\u001b[0m                            for p in polymorphic_type.parameters.values())\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"crop_model = tuner.get_best_models(1)[0]\n\nhistory = crop_model.fit(\n    train_crop,\n    validation_data=val_crop,\n    epochs=20\n)\n\ncrop_model.save(\"crop_model.keras\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T08:07:24.130927Z","iopub.execute_input":"2026-02-16T08:07:24.131482Z","iopub.status.idle":"2026-02-16T08:29:23.539345Z","shell.execute_reply.started":"2026-02-16T08:07:24.131449Z","shell.execute_reply":"2026-02-16T08:29:23.538508Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 432 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 118ms/step - accuracy: 0.9985 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 1.2221e-04\nEpoch 2/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - accuracy: 0.9996 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 8.9335e-05\nEpoch 3/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 71ms/step - accuracy: 0.9995 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 6.7171e-06\nEpoch 4/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 71ms/step - accuracy: 0.9996 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 0.0025\nEpoch 5/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - accuracy: 0.9982 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 8.3184e-06\nEpoch 6/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 1.7092e-04 - val_accuracy: 1.0000 - val_loss: 5.4604e-06\nEpoch 7/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 4.1057e-05 - val_accuracy: 1.0000 - val_loss: 1.5716e-06\nEpoch 8/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - accuracy: 0.9997 - loss: 7.0652e-04 - val_accuracy: 1.0000 - val_loss: 6.2065e-05\nEpoch 9/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - accuracy: 0.9992 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 2.1804e-05\nEpoch 10/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 6.5951e-05 - val_accuracy: 1.0000 - val_loss: 2.8000e-06\nEpoch 11/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 4.2411e-05 - val_accuracy: 1.0000 - val_loss: 6.0104e-06\nEpoch 12/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 69ms/step - accuracy: 0.9998 - loss: 7.5544e-04 - val_accuracy: 1.0000 - val_loss: 1.7569e-05\nEpoch 13/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - accuracy: 0.9999 - loss: 3.1402e-04 - val_accuracy: 1.0000 - val_loss: 9.7921e-05\nEpoch 14/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 69ms/step - accuracy: 0.9989 - loss: 0.0103 - val_accuracy: 1.0000 - val_loss: 1.2213e-05\nEpoch 15/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 71ms/step - accuracy: 0.9997 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 8.7424e-05\nEpoch 16/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 7.1637e-05 - val_accuracy: 1.0000 - val_loss: 8.5447e-06\nEpoch 17/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 69ms/step - accuracy: 0.9996 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 2.2378e-05\nEpoch 18/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - accuracy: 0.9994 - loss: 0.0017 - val_accuracy: 1.0000 - val_loss: 5.7449e-06\nEpoch 19/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 4.3250e-05 - val_accuracy: 1.0000 - val_loss: 8.1903e-07\nEpoch 20/20\n\u001b[1m865/865\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 70ms/step - accuracy: 0.9997 - loss: 0.0019 - val_accuracy: 1.0000 - val_loss: 7.4406e-06\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"loss, acc = crop_model.evaluate(test_crop)\nprint(\"FINAL REAL-WORLD ACCURACY:\", acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T08:29:32.524556Z","iopub.execute_input":"2026-02-16T08:29:32.524853Z","iopub.status.idle":"2026-02-16T08:29:40.428728Z","shell.execute_reply.started":"2026-02-16T08:29:32.524828Z","shell.execute_reply":"2026-02-16T08:29:40.428159Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 182ms/step - accuracy: 1.0000 - loss: 3.8961e-06\nFINAL REAL-WORLD ACCURACY: 1.0\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def load_disease_ds(crop):\n\n    train = tf.keras.utils.image_dataset_from_directory(\n        f\"/kaggle/working/working_dataset/train/{crop}\",\n        image_size=(IMG,IMG),\n        batch_size=BATCH\n    )\n    class_names = train.class_names\n\n    val = tf.keras.utils.image_dataset_from_directory(\n        f\"/kaggle/working/working_dataset/val/{crop}\",\n        image_size=(IMG,IMG),\n        batch_size=BATCH\n    )\n\n    test = tf.keras.utils.image_dataset_from_directory(\n        f\"/kaggle/working/working_dataset/test/{crop}\",\n        image_size=(IMG,IMG),\n        batch_size=BATCH,\n        shuffle=False\n    )\n\n    # apply performance optimization AFTER metadata extraction\n    train = train.prefetch(AUTOTUNE)\n    val   = val.prefetch(AUTOTUNE)\n    test  = test.prefetch(AUTOTUNE)\n\n    return train, val, test, class_names\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T08:53:32.766196Z","iopub.execute_input":"2026-02-16T08:53:32.767147Z","iopub.status.idle":"2026-02-16T08:53:32.772445Z","shell.execute_reply.started":"2026-02-16T08:53:32.767065Z","shell.execute_reply":"2026-02-16T08:53:32.771616Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from tensorflow.keras import layers, Model\nimport tensorflow as tf\n\ndef build_disease_model(backbone_name, num_classes):\n\n    if backbone_name == \"efficientnet\":\n        base = tf.keras.applications.EfficientNetB0(\n            include_top=False,\n            weights=\"imagenet\",\n            input_shape=(IMG,IMG,3)\n        )\n\n    elif backbone_name == \"mobilenet\":\n        base = tf.keras.applications.MobileNetV3Large(\n            include_top=False,\n            weights=\"imagenet\",\n            input_shape=(IMG,IMG,3)\n        )\n\n    else:\n        raise ValueError(\"Unknown backbone\")\n\n    x = layers.GlobalAveragePooling2D()(base.output)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dense(128, activation=\"relu\")(x)\n    x = layers.Dropout(0.5)(x)\n\n    logits = layers.Dense(num_classes)(x)\n\n    model = Model(base.input, logits)\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(1e-4),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[\"accuracy\"]\n    )\n\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import layers, Model\nimport tensorflow as tf\n\ndef build_disease_model(backbone_name, num_classes):\n\n    if backbone_name == \"efficientnet\":\n        base = tf.keras.applications.EfficientNetB0(\n            include_top=False,\n            weights=\"imagenet\",\n            input_shape=(IMG,IMG,3)\n        )\n\n    elif backbone_name == \"mobilenet\":\n        base = tf.keras.applications.MobileNetV3Large(\n            include_top=False,\n            weights=\"imagenet\",\n            input_shape=(IMG,IMG,3)\n        )\n\n    else:\n        raise ValueError(\"Unknown backbone\")\n\n    # Freeze first (important for small agricultural dataset)\n    base.trainable = False\n\n    x = layers.GlobalAveragePooling2D()(base.output)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dense(128, activation=\"relu\")(x)\n    x = layers.Dropout(0.5)(x)\n\n    logits = layers.Dense(num_classes)(x)\n\n    model = Model(base.input, logits)\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(1e-4),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[\"accuracy\"]\n    )\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T08:54:25.757063Z","iopub.execute_input":"2026-02-16T08:54:25.757683Z","iopub.status.idle":"2026-02-16T08:54:25.764393Z","shell.execute_reply.started":"2026-02-16T08:54:25.757652Z","shell.execute_reply":"2026-02-16T08:54:25.763721Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"results = {}\n\nfor crop in CROP_NAMES:\n\n    print(\"\\n==============================\")\n    print(\"CROP:\", crop)\n    print(\"==============================\")\n\n    train, val, test, classes = load_disease_ds(crop)\n    print(\"Classes:\", classes)\n\n    results[crop] = {}\n\n    for backbone in [\"efficientnet\", \"mobilenet\"]:\n\n        print(\"\\nTraining\", backbone)\n\n        model = build_disease_model(backbone, len(classes))\n\n        history = model.fit(\n            train,\n            validation_data=val,\n            epochs=12,\n            verbose=1\n        )\n\n        loss, acc = model.evaluate(test, verbose=0)\n\n        print(\"Test Accuracy:\", acc)\n\n        results[crop][backbone] = acc\n\n        model.save(f\"{crop}_{backbone}.keras\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T08:54:44.487875Z","iopub.execute_input":"2026-02-16T08:54:44.488283Z","iopub.status.idle":"2026-02-16T09:07:05.163828Z","shell.execute_reply.started":"2026-02-16T08:54:44.488255Z","shell.execute_reply":"2026-02-16T09:07:05.163021Z"}},"outputs":[{"name":"stdout","text":"\n==============================\nCROP: Bitter gourd\n==============================\nFound 967 files belonging to 3 classes.\nFound 207 files belonging to 3 classes.\nFound 209 files belonging to 3 classes.\nClasses: ['Anthracnose', 'Downey Mildew', 'Healthy']\n\nTraining efficientnet\nEpoch 1/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 388ms/step - accuracy: 0.5010 - loss: 1.2448 - val_accuracy: 0.9130 - val_loss: 0.6308\nEpoch 2/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.8022 - loss: 0.5123 - val_accuracy: 0.9179 - val_loss: 0.4226\nEpoch 3/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.8289 - loss: 0.4555 - val_accuracy: 0.9275 - val_loss: 0.2971\nEpoch 4/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.8772 - loss: 0.3522 - val_accuracy: 0.9372 - val_loss: 0.2268\nEpoch 5/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.8863 - loss: 0.3135 - val_accuracy: 0.9517 - val_loss: 0.1761\nEpoch 6/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.8792 - loss: 0.2925 - val_accuracy: 0.9517 - val_loss: 0.1537\nEpoch 7/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9082 - loss: 0.2343 - val_accuracy: 0.9517 - val_loss: 0.1375\nEpoch 8/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.9015 - loss: 0.2453 - val_accuracy: 0.9614 - val_loss: 0.1203\nEpoch 9/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9113 - loss: 0.2612 - val_accuracy: 0.9710 - val_loss: 0.1046\nEpoch 10/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.9356 - loss: 0.1870 - val_accuracy: 0.9710 - val_loss: 0.1024\nEpoch 11/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9345 - loss: 0.1792 - val_accuracy: 0.9710 - val_loss: 0.0948\nEpoch 12/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9409 - loss: 0.1679 - val_accuracy: 0.9662 - val_loss: 0.0950\nTest Accuracy: 0.9760765433311462\n\nTraining mobilenet\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\n\u001b[1m12683000/12683000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEpoch 1/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 306ms/step - accuracy: 0.5175 - loss: 1.3149 - val_accuracy: 0.8502 - val_loss: 0.4589\nEpoch 2/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7454 - loss: 0.6780 - val_accuracy: 0.9275 - val_loss: 0.3045\nEpoch 3/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8335 - loss: 0.4317 - val_accuracy: 0.9372 - val_loss: 0.2323\nEpoch 4/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8772 - loss: 0.3349 - val_accuracy: 0.9469 - val_loss: 0.1943\nEpoch 5/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8947 - loss: 0.2701 - val_accuracy: 0.9565 - val_loss: 0.1720\nEpoch 6/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9190 - loss: 0.2521 - val_accuracy: 0.9710 - val_loss: 0.1577\nEpoch 7/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9239 - loss: 0.2150 - val_accuracy: 0.9662 - val_loss: 0.1471\nEpoch 8/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9371 - loss: 0.1836 - val_accuracy: 0.9662 - val_loss: 0.1405\nEpoch 9/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9297 - loss: 0.2133 - val_accuracy: 0.9614 - val_loss: 0.1358\nEpoch 10/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9322 - loss: 0.2066 - val_accuracy: 0.9662 - val_loss: 0.1323\nEpoch 11/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9492 - loss: 0.1520 - val_accuracy: 0.9662 - val_loss: 0.1300\nEpoch 12/12\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.9586 - loss: 0.1206 - val_accuracy: 0.9662 - val_loss: 0.1268\nTest Accuracy: 0.9712918400764465\n\n==============================\nCROP: Okra\n==============================\nFound 6167 files belonging to 2 classes.\nFound 162 files belonging to 2 classes.\nFound 163 files belonging to 2 classes.\nClasses: ['Cerospora Leaf Spot', 'Healthy']\n\nTraining efficientnet\nEpoch 1/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.8302 - loss: 0.3767 - val_accuracy: 0.9877 - val_loss: 0.0446\nEpoch 2/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.9621 - loss: 0.0986 - val_accuracy: 0.9877 - val_loss: 0.0182\nEpoch 3/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9751 - loss: 0.0686 - val_accuracy: 1.0000 - val_loss: 0.0071\nEpoch 4/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9853 - loss: 0.0458 - val_accuracy: 1.0000 - val_loss: 0.0044\nEpoch 5/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9852 - loss: 0.0402 - val_accuracy: 1.0000 - val_loss: 0.0020\nEpoch 6/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9931 - loss: 0.0245 - val_accuracy: 1.0000 - val_loss: 0.0012\nEpoch 7/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9908 - loss: 0.0267 - val_accuracy: 1.0000 - val_loss: 0.0017\nEpoch 8/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9844 - loss: 0.0350 - val_accuracy: 1.0000 - val_loss: 7.3834e-04\nEpoch 9/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9918 - loss: 0.0205 - val_accuracy: 1.0000 - val_loss: 3.1825e-04\nEpoch 10/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.9935 - loss: 0.0170 - val_accuracy: 1.0000 - val_loss: 4.6139e-04\nEpoch 11/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.9926 - loss: 0.0238 - val_accuracy: 1.0000 - val_loss: 3.5409e-04\nEpoch 12/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.9919 - loss: 0.0235 - val_accuracy: 1.0000 - val_loss: 1.9120e-04\nTest Accuracy: 1.0\n\nTraining mobilenet\nEpoch 1/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 51ms/step - accuracy: 0.8342 - loss: 0.3661 - val_accuracy: 1.0000 - val_loss: 0.0304\nEpoch 2/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.9643 - loss: 0.0933 - val_accuracy: 1.0000 - val_loss: 0.0129\nEpoch 3/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.9782 - loss: 0.0610 - val_accuracy: 1.0000 - val_loss: 0.0067\nEpoch 4/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.9859 - loss: 0.0442 - val_accuracy: 1.0000 - val_loss: 0.0033\nEpoch 5/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.9870 - loss: 0.0378 - val_accuracy: 1.0000 - val_loss: 0.0022\nEpoch 6/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.9906 - loss: 0.0272 - val_accuracy: 1.0000 - val_loss: 0.0016\nEpoch 7/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.9924 - loss: 0.0230 - val_accuracy: 1.0000 - val_loss: 0.0010\nEpoch 8/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.9923 - loss: 0.0222 - val_accuracy: 1.0000 - val_loss: 8.2686e-04\nEpoch 9/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.9935 - loss: 0.0169 - val_accuracy: 1.0000 - val_loss: 5.3608e-04\nEpoch 10/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.9955 - loss: 0.0137 - val_accuracy: 1.0000 - val_loss: 3.7230e-04\nEpoch 11/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.9955 - loss: 0.0119 - val_accuracy: 1.0000 - val_loss: 3.2367e-04\nEpoch 12/12\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.9978 - loss: 0.0084 - val_accuracy: 1.0000 - val_loss: 2.0793e-04\nTest Accuracy: 0.9938650131225586\n\n==============================\nCROP: Pumpkin\n==============================\nFound 5967 files belonging to 2 classes.\nFound 157 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nClasses: ['Downey Mildew', 'Healthy']\n\nTraining efficientnet\nEpoch 1/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 76ms/step - accuracy: 0.8059 - loss: 0.4545 - val_accuracy: 0.9427 - val_loss: 0.1032\nEpoch 2/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9337 - loss: 0.1709 - val_accuracy: 0.9618 - val_loss: 0.0616\nEpoch 3/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9444 - loss: 0.1380 - val_accuracy: 0.9745 - val_loss: 0.0489\nEpoch 4/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.9546 - loss: 0.1136 - val_accuracy: 0.9936 - val_loss: 0.0370\nEpoch 5/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.9674 - loss: 0.0822 - val_accuracy: 0.9936 - val_loss: 0.0283\nEpoch 6/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.9715 - loss: 0.0805 - val_accuracy: 0.9936 - val_loss: 0.0241\nEpoch 7/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9736 - loss: 0.0707 - val_accuracy: 1.0000 - val_loss: 0.0189\nEpoch 8/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.9721 - loss: 0.0797 - val_accuracy: 1.0000 - val_loss: 0.0145\nEpoch 9/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9790 - loss: 0.0599 - val_accuracy: 0.9936 - val_loss: 0.0168\nEpoch 10/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9790 - loss: 0.0517 - val_accuracy: 1.0000 - val_loss: 0.0156\nEpoch 11/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.9799 - loss: 0.0533 - val_accuracy: 1.0000 - val_loss: 0.0145\nEpoch 12/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9824 - loss: 0.0498 - val_accuracy: 0.9936 - val_loss: 0.0156\nTest Accuracy: 0.9936708807945251\n\nTraining mobilenet\nEpoch 1/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 56ms/step - accuracy: 0.7774 - loss: 0.5091 - val_accuracy: 0.9490 - val_loss: 0.1204\nEpoch 2/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.9395 - loss: 0.1564 - val_accuracy: 0.9618 - val_loss: 0.1002\nEpoch 3/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.9537 - loss: 0.1170 - val_accuracy: 0.9682 - val_loss: 0.0856\nEpoch 4/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.9721 - loss: 0.0829 - val_accuracy: 0.9682 - val_loss: 0.0730\nEpoch 5/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.9664 - loss: 0.0847 - val_accuracy: 0.9809 - val_loss: 0.0594\nEpoch 6/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.9712 - loss: 0.0765 - val_accuracy: 0.9873 - val_loss: 0.0435\nEpoch 7/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.9811 - loss: 0.0544 - val_accuracy: 0.9873 - val_loss: 0.0404\nEpoch 8/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.9798 - loss: 0.0540 - val_accuracy: 0.9873 - val_loss: 0.0254\nEpoch 9/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.9833 - loss: 0.0489 - val_accuracy: 0.9936 - val_loss: 0.0247\nEpoch 10/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.9808 - loss: 0.0495 - val_accuracy: 0.9936 - val_loss: 0.0194\nEpoch 11/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.9849 - loss: 0.0437 - val_accuracy: 1.0000 - val_loss: 0.0138\nEpoch 12/12\n\u001b[1m373/373\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.9839 - loss: 0.0463 - val_accuracy: 1.0000 - val_loss: 0.0120\nTest Accuracy: 0.9873417615890503\n\n==============================\nCROP: Ridge gourd\n==============================\nFound 738 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nFound 160 files belonging to 2 classes.\nClasses: ['Downey Mildew', 'Healthy']\n\nTraining efficientnet\nEpoch 1/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 444ms/step - accuracy: 0.7840 - loss: 0.5694 - val_accuracy: 0.9367 - val_loss: 0.3592\nEpoch 2/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.9217 - loss: 0.1954 - val_accuracy: 0.9810 - val_loss: 0.2417\nEpoch 3/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - accuracy: 0.9431 - loss: 0.1229 - val_accuracy: 0.9810 - val_loss: 0.1641\nEpoch 4/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.9621 - loss: 0.0872 - val_accuracy: 0.9873 - val_loss: 0.1123\nEpoch 5/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - accuracy: 0.9701 - loss: 0.0826 - val_accuracy: 1.0000 - val_loss: 0.0756\nEpoch 6/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 66ms/step - accuracy: 0.9683 - loss: 0.0795 - val_accuracy: 1.0000 - val_loss: 0.0519\nEpoch 7/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.9878 - loss: 0.0492 - val_accuracy: 1.0000 - val_loss: 0.0358\nEpoch 8/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - accuracy: 0.9869 - loss: 0.0375 - val_accuracy: 1.0000 - val_loss: 0.0273\nEpoch 9/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - accuracy: 0.9821 - loss: 0.0540 - val_accuracy: 1.0000 - val_loss: 0.0223\nEpoch 10/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.9876 - loss: 0.0453 - val_accuracy: 1.0000 - val_loss: 0.0167\nEpoch 11/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.9927 - loss: 0.0283 - val_accuracy: 1.0000 - val_loss: 0.0139\nEpoch 12/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - accuracy: 0.9922 - loss: 0.0312 - val_accuracy: 1.0000 - val_loss: 0.0135\nTest Accuracy: 1.0\n\nTraining mobilenet\nEpoch 1/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 330ms/step - accuracy: 0.6506 - loss: 0.7112 - val_accuracy: 0.8038 - val_loss: 0.3609\nEpoch 2/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.9236 - loss: 0.1755 - val_accuracy: 0.9367 - val_loss: 0.1798\nEpoch 3/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.9489 - loss: 0.1154 - val_accuracy: 0.9873 - val_loss: 0.1023\nEpoch 4/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.9593 - loss: 0.1028 - val_accuracy: 0.9937 - val_loss: 0.0697\nEpoch 5/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.9860 - loss: 0.0583 - val_accuracy: 0.9937 - val_loss: 0.0517\nEpoch 6/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - accuracy: 0.9876 - loss: 0.0503 - val_accuracy: 0.9937 - val_loss: 0.0412\nEpoch 7/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.9906 - loss: 0.0324 - val_accuracy: 0.9937 - val_loss: 0.0365\nEpoch 8/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - accuracy: 0.9875 - loss: 0.0566 - val_accuracy: 0.9937 - val_loss: 0.0356\nEpoch 9/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - accuracy: 0.9939 - loss: 0.0293 - val_accuracy: 0.9937 - val_loss: 0.0337\nEpoch 10/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.9933 - loss: 0.0205 - val_accuracy: 0.9937 - val_loss: 0.0310\nEpoch 11/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - accuracy: 0.9878 - loss: 0.0354 - val_accuracy: 0.9937 - val_loss: 0.0326\nEpoch 12/12\n\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.9968 - loss: 0.0214 - val_accuracy: 0.9937 - val_loss: 0.0320\nTest Accuracy: 1.0\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"print(\"\\n\\nFINAL RESULTS\")\nprint(\"=================================\")\n\nfor crop in results:\n    print(f\"\\n{crop}\")\n    for model_name in results[crop]:\n        print(f\"{model_name}: {results[crop][model_name]:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T09:09:19.779948Z","iopub.execute_input":"2026-02-16T09:09:19.780714Z","iopub.status.idle":"2026-02-16T09:09:19.785433Z","shell.execute_reply.started":"2026-02-16T09:09:19.780681Z","shell.execute_reply":"2026-02-16T09:09:19.784749Z"}},"outputs":[{"name":"stdout","text":"\n\nFINAL RESULTS\n=================================\n\nBitter gourd\nefficientnet: 0.9761\nmobilenet: 0.9713\n\nOkra\nefficientnet: 1.0000\nmobilenet: 0.9939\n\nPumpkin\nefficientnet: 0.9937\nmobilenet: 0.9873\n\nRidge gourd\nefficientnet: 1.0000\nmobilenet: 1.0000\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import random\nimport tensorflow as tf\n\ndef cross_crop_test():\n\n    for source_crop in CROP_NAMES:\n\n        _,_,test,_ = load_disease_ds(source_crop)\n        img,label = next(iter(test.take(1)))\n\n        wrong_crop = random.choice([c for c in CROP_NAMES if c != source_crop])\n        wrong_model = tf.keras.models.load_model(f\"{wrong_crop}_efficientnet.keras\")\n\n        pred = wrong_model(img, training=False)\n        conf = tf.nn.softmax(pred)[0].numpy().max()\n\n        print(f\"\\nActual crop: {source_crop}\")\n        print(f\"Tested on model: {wrong_crop}\")\n        print(f\"Confidence:\", conf)\n\ncross_crop_test()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T09:13:06.932058Z","iopub.execute_input":"2026-02-16T09:13:06.932679Z","iopub.status.idle":"2026-02-16T09:13:14.326846Z","shell.execute_reply.started":"2026-02-16T09:13:06.932648Z","shell.execute_reply":"2026-02-16T09:13:14.326221Z"}},"outputs":[{"name":"stdout","text":"Found 967 files belonging to 3 classes.\nFound 207 files belonging to 3 classes.\nFound 209 files belonging to 3 classes.\n\nActual crop: Bitter gourd\nTested on model: Pumpkin\nConfidence: 1.0\nFound 6167 files belonging to 2 classes.\nFound 162 files belonging to 2 classes.\nFound 163 files belonging to 2 classes.\n\nActual crop: Okra\nTested on model: Pumpkin\nConfidence: 0.96768314\nFound 5967 files belonging to 2 classes.\nFound 157 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\n\nActual crop: Pumpkin\nTested on model: Okra\nConfidence: 0.99999726\nFound 738 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nFound 160 files belonging to 2 classes.\n\nActual crop: Ridge gourd\nTested on model: Bitter gourd\nConfidence: 0.99566615\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\ndef energy_score(logits):\n    return -tf.reduce_logsumexp(logits, axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T09:14:13.002019Z","iopub.execute_input":"2026-02-16T09:14:13.002822Z","iopub.status.idle":"2026-02-16T09:14:13.006408Z","shell.execute_reply.started":"2026-02-16T09:14:13.002793Z","shell.execute_reply":"2026-02-16T09:14:13.005817Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def collect_energy(model, dataset):\n\n    energies = []\n\n    for img,label in dataset:\n        logits = model(img, training=False)\n        e = energy_score(logits)\n        energies.extend(e.numpy())\n\n    return np.array(energies)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T09:14:24.109946Z","iopub.execute_input":"2026-02-16T09:14:24.110499Z","iopub.status.idle":"2026-02-16T09:14:24.114454Z","shell.execute_reply.started":"2026-02-16T09:14:24.110469Z","shell.execute_reply":"2026-02-16T09:14:24.113884Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"thresholds = {}\n\nfor crop in CROP_NAMES:\n\n    print(\"\\nCalibrating:\",crop)\n\n    _, val_ds, _, _ = load_disease_ds(crop)\n    model = tf.keras.models.load_model(f\"{crop}_efficientnet.keras\")\n\n    energy_vals = collect_energy(model, val_ds)\n\n    threshold = np.percentile(energy_vals, 95)  # allow 95% known acceptance\n    thresholds[crop] = threshold\n\n    print(\"Threshold:\",threshold)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T09:14:33.182668Z","iopub.execute_input":"2026-02-16T09:14:33.183371Z","iopub.status.idle":"2026-02-16T09:14:54.293479Z","shell.execute_reply.started":"2026-02-16T09:14:33.183340Z","shell.execute_reply":"2026-02-16T09:14:54.292666Z"}},"outputs":[{"name":"stdout","text":"\nCalibrating: Bitter gourd\nFound 967 files belonging to 3 classes.\nFound 207 files belonging to 3 classes.\nFound 209 files belonging to 3 classes.\nThreshold: -2.0138755\n\nCalibrating: Okra\nFound 6167 files belonging to 2 classes.\nFound 162 files belonging to 2 classes.\nFound 163 files belonging to 2 classes.\nThreshold: -2.0678878\n\nCalibrating: Pumpkin\nFound 5967 files belonging to 2 classes.\nFound 157 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nThreshold: -1.4015038\n\nCalibrating: Ridge gourd\nFound 738 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nFound 160 files belonging to 2 classes.\nThreshold: -2.0293026\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def predict_safe(image):\n\n    crop_probs = crop_model.predict(image)\n    crop_id = crop_probs.argmax()\n    crop = CROP_NAMES[crop_id]\n\n    model = tf.keras.models.load_model(f\"{crop}_efficientnet.keras\")\n\n    logits = model(image, training=False)\n    e = energy_score(logits).numpy()[0]\n\n    if e > thresholds[crop]:\n        return crop, \"Unknown Disease\"\n\n    disease_id = tf.argmax(logits,axis=1).numpy()[0]\n    return crop, disease_id\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T09:15:26.106266Z","iopub.execute_input":"2026-02-16T09:15:26.106577Z","iopub.status.idle":"2026-02-16T09:15:26.111540Z","shell.execute_reply.started":"2026-02-16T09:15:26.106550Z","shell.execute_reply":"2026-02-16T09:15:26.110860Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def cross_crop_safe_test():\n\n    for source_crop in CROP_NAMES:\n\n        _,_,test,_ = load_disease_ds(source_crop)\n        img,label = next(iter(test.take(1)))\n\n        crop, disease = predict_safe(img)\n\n        print(\"\\nActual:\",source_crop)\n        print(\"Prediction:\",crop,disease)\n\ncross_crop_safe_test()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T09:15:29.956341Z","iopub.execute_input":"2026-02-16T09:15:29.957216Z","iopub.status.idle":"2026-02-16T09:15:43.012760Z","shell.execute_reply.started":"2026-02-16T09:15:29.957181Z","shell.execute_reply":"2026-02-16T09:15:43.012004Z"}},"outputs":[{"name":"stdout","text":"Found 967 files belonging to 3 classes.\nFound 207 files belonging to 3 classes.\nFound 209 files belonging to 3 classes.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n\nActual: Bitter gourd\nPrediction: Bitter gourd 1\nFound 6167 files belonging to 2 classes.\nFound 162 files belonging to 2 classes.\nFound 163 files belonging to 2 classes.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n\nActual: Okra\nPrediction: Okra 0\nFound 5967 files belonging to 2 classes.\nFound 157 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n\nActual: Pumpkin\nPrediction: Pumpkin 0\nFound 738 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nFound 160 files belonging to 2 classes.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n\nActual: Ridge gourd\nPrediction: Ridge gourd 0\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def collect_energy_known_unknown(crop):\n\n    model = tf.keras.models.load_model(f\"{crop}_efficientnet.keras\")\n\n    # known samples\n    _, val_known, _, _ = load_disease_ds(crop)\n    known_energy = collect_energy(model, val_known)\n\n    # unknown samples (other crops)\n    unknown_energy = []\n\n    for other in CROP_NAMES:\n        if other == crop:\n            continue\n\n        _, val_other, _, _ = load_disease_ds(other)\n\n        for img,_ in val_other:\n            logits = model(img, training=False)\n            e = energy_score(logits)\n            unknown_energy.extend(e.numpy())\n\n    return np.array(known_energy), np.array(unknown_energy)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T09:16:39.553695Z","iopub.execute_input":"2026-02-16T09:16:39.554560Z","iopub.status.idle":"2026-02-16T09:16:39.559870Z","shell.execute_reply.started":"2026-02-16T09:16:39.554528Z","shell.execute_reply":"2026-02-16T09:16:39.559174Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"thresholds = {}\n\nfor crop in CROP_NAMES:\n\n    print(\"\\nCalibrating:\",crop)\n\n    known, unknown = collect_energy_known_unknown(crop)\n\n    print(\"Known avg:\",known.mean())\n    print(\"Unknown avg:\",unknown.mean())\n\n    # midpoint between distributions\n    threshold = (known.mean() + unknown.mean()) / 2\n    thresholds[crop] = threshold\n\n    print(\"Chosen threshold:\",threshold)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T09:16:49.450123Z","iopub.execute_input":"2026-02-16T09:16:49.450844Z","iopub.status.idle":"2026-02-16T09:17:47.851816Z","shell.execute_reply.started":"2026-02-16T09:16:49.450818Z","shell.execute_reply":"2026-02-16T09:17:47.851126Z"}},"outputs":[{"name":"stdout","text":"\nCalibrating: Bitter gourd\nFound 967 files belonging to 3 classes.\nFound 207 files belonging to 3 classes.\nFound 209 files belonging to 3 classes.\nFound 6167 files belonging to 2 classes.\nFound 162 files belonging to 2 classes.\nFound 163 files belonging to 2 classes.\nFound 5967 files belonging to 2 classes.\nFound 157 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nFound 738 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nFound 160 files belonging to 2 classes.\nKnown avg: -4.3829503\nUnknown avg: -3.1536016\nChosen threshold: -3.768276\n\nCalibrating: Okra\nFound 6167 files belonging to 2 classes.\nFound 162 files belonging to 2 classes.\nFound 163 files belonging to 2 classes.\nFound 967 files belonging to 3 classes.\nFound 207 files belonging to 3 classes.\nFound 209 files belonging to 3 classes.\nFound 5967 files belonging to 2 classes.\nFound 157 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nFound 738 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nFound 160 files belonging to 2 classes.\nKnown avg: -5.7612023\nUnknown avg: -2.1095176\nChosen threshold: -3.93536\n\nCalibrating: Pumpkin\nFound 5967 files belonging to 2 classes.\nFound 157 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nFound 967 files belonging to 3 classes.\nFound 207 files belonging to 3 classes.\nFound 209 files belonging to 3 classes.\nFound 6167 files belonging to 2 classes.\nFound 162 files belonging to 2 classes.\nFound 163 files belonging to 2 classes.\nFound 738 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nFound 160 files belonging to 2 classes.\nKnown avg: -5.1577435\nUnknown avg: -4.647258\nChosen threshold: -4.9025006\n\nCalibrating: Ridge gourd\nFound 738 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nFound 160 files belonging to 2 classes.\nFound 967 files belonging to 3 classes.\nFound 207 files belonging to 3 classes.\nFound 209 files belonging to 3 classes.\nFound 6167 files belonging to 2 classes.\nFound 162 files belonging to 2 classes.\nFound 163 files belonging to 2 classes.\nFound 5967 files belonging to 2 classes.\nFound 157 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nKnown avg: -4.364016\nUnknown avg: -3.1155515\nChosen threshold: -3.7397838\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"def predict_safe(image):\n\n    crop_probs = crop_model.predict(image)\n    crop_id = crop_probs.argmax()\n    crop = CROP_NAMES[crop_id]\n\n    model = tf.keras.models.load_model(f\"{crop}_efficientnet.keras\")\n    classes = load_disease_ds(crop)[3]\n\n    logits = model(image, training=False)\n    e = energy_score(logits).numpy()[0]\n\n    if e > thresholds[crop]:\n        return crop, \"Unknown Disease\"\n\n    disease_id = tf.argmax(logits,axis=1).numpy()[0]\n    return crop, classes[disease_id]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T09:17:52.499817Z","iopub.execute_input":"2026-02-16T09:17:52.500492Z","iopub.status.idle":"2026-02-16T09:17:52.505582Z","shell.execute_reply.started":"2026-02-16T09:17:52.500461Z","shell.execute_reply":"2026-02-16T09:17:52.504796Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"cross_crop_safe_test()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T09:17:56.058749Z","iopub.execute_input":"2026-02-16T09:17:56.059392Z","iopub.status.idle":"2026-02-16T09:18:03.774146Z","shell.execute_reply.started":"2026-02-16T09:17:56.059363Z","shell.execute_reply":"2026-02-16T09:18:03.773363Z"}},"outputs":[{"name":"stdout","text":"Found 967 files belonging to 3 classes.\nFound 207 files belonging to 3 classes.\nFound 209 files belonging to 3 classes.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\nFound 967 files belonging to 3 classes.\nFound 207 files belonging to 3 classes.\nFound 209 files belonging to 3 classes.\n\nActual: Bitter gourd\nPrediction: Bitter gourd Downey Mildew\nFound 6167 files belonging to 2 classes.\nFound 162 files belonging to 2 classes.\nFound 163 files belonging to 2 classes.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\nFound 6167 files belonging to 2 classes.\nFound 162 files belonging to 2 classes.\nFound 163 files belonging to 2 classes.\n\nActual: Okra\nPrediction: Okra Cerospora Leaf Spot\nFound 5967 files belonging to 2 classes.\nFound 157 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\nFound 5967 files belonging to 2 classes.\nFound 157 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\n\nActual: Pumpkin\nPrediction: Pumpkin Downey Mildew\nFound 738 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nFound 160 files belonging to 2 classes.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\nFound 738 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nFound 160 files belonging to 2 classes.\n\nActual: Ridge gourd\nPrediction: Ridge gourd Downey Mildew\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from tensorflow.keras import Model\n\ndef get_feature_extractor(model):\n    return Model(model.input, model.layers[-3].output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T09:18:55.722824Z","iopub.execute_input":"2026-02-16T09:18:55.723606Z","iopub.status.idle":"2026-02-16T09:18:55.727520Z","shell.execute_reply.started":"2026-02-16T09:18:55.723565Z","shell.execute_reply":"2026-02-16T09:18:55.726951Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import numpy as np\n\ndef compute_prototypes(model, dataset, num_classes):\n\n    extractor = get_feature_extractor(model)\n\n    feats = [[] for _ in range(num_classes)]\n\n    for img, label in dataset:\n        emb = extractor(img, training=False).numpy()\n        for i,l in enumerate(label.numpy()):\n            feats[l].append(emb[i])\n\n    prototypes = [np.mean(f, axis=0) for f in feats]\n    return np.array(prototypes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T09:19:08.804120Z","iopub.execute_input":"2026-02-16T09:19:08.804708Z","iopub.status.idle":"2026-02-16T09:19:08.809493Z","shell.execute_reply.started":"2026-02-16T09:19:08.804679Z","shell.execute_reply":"2026-02-16T09:19:08.808820Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from scipy.spatial.distance import cdist\n\nproto_bank = {}\ndist_thresholds = {}\n\nfor crop in CROP_NAMES:\n\n    print(\"\\nCalibrating:\", crop)\n\n    train, val, _, classes = load_disease_ds(crop)\n    model = tf.keras.models.load_model(f\"{crop}_efficientnet.keras\")\n\n    prototypes = compute_prototypes(model, train, len(classes))\n    proto_bank[crop] = prototypes\n\n    extractor = get_feature_extractor(model)\n\n    dists = []\n\n    for img,_ in val:\n        emb = extractor(img, training=False).numpy()\n        dist = cdist(emb, prototypes).min(axis=1)\n        dists.extend(dist)\n\n    threshold = np.percentile(dists, 95)\n    dist_thresholds[crop] = threshold\n\n    print(\"Threshold:\", threshold)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T09:19:29.230466Z","iopub.execute_input":"2026-02-16T09:19:29.230768Z","iopub.status.idle":"2026-02-16T09:23:39.917003Z","shell.execute_reply.started":"2026-02-16T09:19:29.230741Z","shell.execute_reply":"2026-02-16T09:23:39.916382Z"}},"outputs":[{"name":"stdout","text":"\nCalibrating: Bitter gourd\nFound 967 files belonging to 3 classes.\nFound 207 files belonging to 3 classes.\nFound 209 files belonging to 3 classes.\nThreshold: 10.934633910785847\n\nCalibrating: Okra\nFound 6167 files belonging to 2 classes.\nFound 162 files belonging to 2 classes.\nFound 163 files belonging to 2 classes.\nThreshold: 14.641745422117197\n\nCalibrating: Pumpkin\nFound 5967 files belonging to 2 classes.\nFound 157 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nThreshold: 16.85634665596395\n\nCalibrating: Ridge gourd\nFound 738 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nFound 160 files belonging to 2 classes.\nThreshold: 9.920366590954618\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"def predict_safe(image):\n\n    crop_probs = crop_model.predict(image)\n    crop_id = crop_probs.argmax()\n    crop = CROP_NAMES[crop_id]\n\n    model = tf.keras.models.load_model(f\"{crop}_efficientnet.keras\")\n    extractor = get_feature_extractor(model)\n    prototypes = proto_bank[crop]\n    classes = load_disease_ds(crop)[3]\n\n    emb = extractor(image, training=False).numpy()\n    dist = cdist(emb, prototypes).min()\n\n    if dist > dist_thresholds[crop]:\n        return crop, \"Unknown Disease\"\n\n    logits = model(image, training=False)\n    disease_id = tf.argmax(logits,axis=1).numpy()[0]\n\n    return crop, classes[disease_id]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T09:24:15.833513Z","iopub.execute_input":"2026-02-16T09:24:15.834273Z","iopub.status.idle":"2026-02-16T09:24:15.839826Z","shell.execute_reply.started":"2026-02-16T09:24:15.834241Z","shell.execute_reply":"2026-02-16T09:24:15.839026Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"cross_crop_safe_test()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T09:24:19.136844Z","iopub.execute_input":"2026-02-16T09:24:19.137454Z","iopub.status.idle":"2026-02-16T09:24:27.691811Z","shell.execute_reply.started":"2026-02-16T09:24:19.137426Z","shell.execute_reply":"2026-02-16T09:24:27.691188Z"}},"outputs":[{"name":"stdout","text":"Found 967 files belonging to 3 classes.\nFound 207 files belonging to 3 classes.\nFound 209 files belonging to 3 classes.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\nFound 967 files belonging to 3 classes.\nFound 207 files belonging to 3 classes.\nFound 209 files belonging to 3 classes.\n\nActual: Bitter gourd\nPrediction: Bitter gourd Downey Mildew\nFound 6167 files belonging to 2 classes.\nFound 162 files belonging to 2 classes.\nFound 163 files belonging to 2 classes.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\nFound 6167 files belonging to 2 classes.\nFound 162 files belonging to 2 classes.\nFound 163 files belonging to 2 classes.\n\nActual: Okra\nPrediction: Okra Cerospora Leaf Spot\nFound 5967 files belonging to 2 classes.\nFound 157 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\nFound 5967 files belonging to 2 classes.\nFound 157 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\n\nActual: Pumpkin\nPrediction: Pumpkin Downey Mildew\nFound 738 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nFound 160 files belonging to 2 classes.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\nFound 738 files belonging to 2 classes.\nFound 158 files belonging to 2 classes.\nFound 160 files belonging to 2 classes.\n\nActual: Ridge gourd\nPrediction: Ridge gourd Downey Mildew\n","output_type":"stream"}],"execution_count":31}]}